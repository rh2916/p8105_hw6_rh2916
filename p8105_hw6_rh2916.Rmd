---
title: "p8105_hw6_rh2916"
author: "Rui Huang"
date: "November 16, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(purrr)
library(modelr)
library(mgcv)
```

## Problem 1 

### Tidy data for analysis

```{r}
df_homicide = read.csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(city_state = paste(city, state, sep = ","),
         sloving_status = ifelse(disposition == "Closed by arrest", 1, 0)) %>%
  filter(city_state != "Dallas,TX" & city_state != "Phoenix,AZ" & city_state != "Kansas City,MO" & city_state != "Tulsa,AL") %>%
  mutate(victim_race = ifelse(victim_race == "White", "white","non_white"),
         victim_race = fct_relevel(victim_race,"white", "non_white"),
         victim_age = as.numeric(victim_age))
```

After cleaning and tidying, the dataset contains `r ncol(df_homicide)` variables and `r nrow(df_homicide)` observations.

### Fit regression for Baltimore,MD

The logistic regression is fitted by glm with resolved vs unresolved as the outcome and victim victim_age, victim_sex and victim_race as predictors. 

```{r}
Baltimore_homicide =
  df_homicide %>% 
  filter(city_state == "Baltimore,MD") 
  
glm_sloving_status = glm(sloving_status ~ victim_age + victim_sex + victim_race, data = Baltimore_homicide, family = binomial())

glm_sloving_status %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         log_OR = estimate,
         OR_lower = exp(estimate - std.error * 1.96),
         OR_upper = exp(estimate + std.error * 1.96)) %>%
  select(term, log_OR, OR, OR_lower, OR_upper, p.value) %>% 
  knitr::kable(digits = 3)
```

From the regression, we can find that homicides of non-whites are much less likely to be solved than white victims. The adjusted odds ratio is 0.453, 95% CI is (0.322, 0.637).


### Run glm for each city 

Write function and loop it by may_df

```{r}
func_solving = function(x){
  
city_homicide = df_homicide %>%
  filter(city_state == x) 
  
glm_sloving_status = glm(sloving_status ~  victim_age + victim_sex + victim_race, data = city_homicide, family = binomial()) 

glm_sloving_status %>% 
  broom::tidy() %>% 
  filter(term == "victim_racenon_white") %>%
  mutate(OR = exp(estimate),
         log_OR = estimate,
         OR_lower = exp(estimate - std.error * 1.96),
         OR_upper = exp(estimate + std.error * 1.96)) %>%
  select(term, log_ORï¼ŒOR, OR_lower, OR_upper, p.value)
}


glm_city = purrr::map_df(.x = unique(df_homicide$city_state), func_solving) %>%
  mutate(term = unique(df_homicide$city_state)) %>%
  select(term, OR, OR_lower, OR_upper)

knitr::kable(glm_city)
```

### Create a plot shows the  ORs and CIs

```{r}
ggplot(glm_city, aes(x=reorder(term, -OR),y=OR))+
  geom_point() +
  geom_errorbar(aes(ymin = OR_lower, ymax = OR_upper)) +
  geom_hline(yintercept = 1, alpha = 0.4) +
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 90, hjust = 1), legend.key.width = unit(0.15,'cm')) +
  labs(
    title = "Adjusted OR for homicide resolvation among non-whites and whites by city",
    x = "City, State",
    y = "Adjusted Odds Ratio with 95% CIs"
  )
  ggtitle('Adjusted Odds Ratio with 95% CIs for each city')
```

The graph is Adjusted Odds Ratio with 95% CIs for each city in decreasing OR. From the graph, we can find that Boston,MA has the least OR while the Tampa,FL has the highest one. Also, except for Tampa,FL, Durham,NC and Birmingham,AL, all the restcities have OR less than 1, which shows higher estimate of solving rate of white people than non-white people. 


## Problem 2

### Load and clean the data for regression analysis 

```{r, message=F, message=F}
df_birthweight_raw = read.csv("./data/birthweight.csv")

df_birthweight = 
  df_birthweight_raw %>% 
  janitor::clean_names() %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         mrace = as.factor(mrace),
         malform = as.factor(malform))

skimr::skim(df_birthweight)
```

### Propose a regression model for birthweight.


```{r, message=F}
full_model = lm(bwt ~ ., data = df_birthweight)
summary(full_model)

step_model = step(full_model, direction = "both", trace = FALSE)
summary(step_model)
```

At first, the model all the possible factors and built model by stepwise selection. From the summary above, we can find that bhead, babysex, blength, delwt, gaweeks, wtgain and smoken are significant, so keep these variables as predictors in the final model. 

### Plot the final model with predictions and residuals

```{r}
lm_model = lm(bwt ~ bhead + babysex + blength + delwt + gaweeks + wtgain + smoken, data = df_birthweight)

df_birthweight %>% 
  modelr::add_predictions(lm_model) %>% 
  modelr::add_residuals(lm_model) %>% 
  ggplot(aes(x=pred,y=resid))+
  geom_point(aes(alpha=.2))+
  labs(
    title = "Residuals vs fitted values of birthweight",
    x = "Predictions", y = "Residuals"
  )
```

The plot is Residuals vs fitted values of birthweight, from which we can find that most of the residuals are around 0, but there are some abnormal values over 1000, which indicates there may be some outliers in the lower range of birthweight.

### Making two more models

main_effect_model contains length at birth and gestational age as predictors, interaction_model contains head circumference, length, sex, and all interactions as predictors

```{r}
main_effect_model = lm(bwt ~ blength + gaweeks, data = df_birthweight)
interaction_model = lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data = df_birthweight)
```

### Compare my model to two others by Cross-validation

```{r}
cv_df = 
  crossv_mc(df_birthweight, 100)

cv_df = cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(lm_mod = map(train,~lm(bwt ~ bhead + babysex + blength + delwt + gaweeks + wtgain + smoken, data=.x)),
         main_effect_mod = map(train, ~lm(bwt~blength+gaweeks,data=.x)),
         interaction_mod = map(train, ~lm(bwt~bhead + blength + babysex + bhead * blength * babysex, data=.x))) %>% 
  mutate(rmse_lm_mod = map2_dbl(lm_mod,test, ~rmse(model = .x, data=.y)),
         rmse_main_effect_mod = map2_dbl(main_effect_mod,test, ~rmse(model = .x, data=.y)),
         rmse_interaction_mod = map2_dbl(interaction_mod,test, ~rmse(model = .x, data=.y))) 
```

### Make plot of RMSEs

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From the plot, we can find that compared to The main_effect_model, the lm_model and interaction_model have lower rmse and lm_model is the best.  

